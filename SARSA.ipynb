{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA:\n",
    "    def __init__(self,state_dim,action_dim,lr=0.01,gamma=0.8,e_greed=0.1):\n",
    "        self.state_dim=state_dim\n",
    "        self.action_dim=action_dim\n",
    "        self.lr=lr\n",
    "        self.gamma=gamma\n",
    "        self.e_greed=e_greed\n",
    "        self.Q=np.zeros((state_dim,action_dim))\n",
    "    def _action(self,state):\n",
    "        if np.random.uniform()<self.e_greed:\n",
    "            action=np.random.choice(self.action_dim)\n",
    "        else:\n",
    "            action=self.predict(state)\n",
    "        return action\n",
    "    def predict(self,state):\n",
    "        all_actions=self.Q[state,:]\n",
    "        max_action=np.max(all_actions)\n",
    "        max_actions_list=np.where(all_actions==max_action)[0]\n",
    "        return np.random.choice(max_actions_list)\n",
    "    def learn(self,state,action,reward,next_state,next_action,done):\n",
    "        if done:\n",
    "            target=reward\n",
    "        else:\n",
    "            target=reward+self.gamma*(self.Q[next_state,next_action])\n",
    "        \n",
    "        self.Q[state,action]+=self.lr*(target-self.Q[state,action])\n",
    "    \n",
    "    def save(self):\n",
    "        npy_file = './model/SARSA_table.npy'\n",
    "        np.save(npy_file, self.Q)\n",
    "        print(npy_file + ' saved.')\n",
    "\n",
    "    def load(self, npy_file='./model/SARSA_table.npy'):\n",
    "        self.Q = np.load(npy_file)\n",
    "        print(npy_file + ' loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,env,lr=0.1,gamma=0.8,e_greed=0.1):\n",
    "        self.env=env\n",
    "        self.lr=lr\n",
    "        self.gamma=gamma\n",
    "        self.e_greed=e_greed\n",
    "        self.model=SARSA(self.env.observation_space.n,self.env.action_space.n,lr,gamma,e_greed)\n",
    "    \n",
    "    def train_eporch(self,render=False):\n",
    "        total_reward=0\n",
    "        total_steps=0\n",
    "        state=self.env.reset()\n",
    "        action=self.model._action(state)\n",
    "        while True:\n",
    "            next_state,reward,done,_=self.env.step(action)\n",
    "            next_action=self.model._action(next_state)\n",
    "            #Training Sarsa method,update Q-table\n",
    "            self.model.learn(state,action,reward,next_state,next_action,done)\n",
    "            total_reward+=reward\n",
    "            total_steps+=1\n",
    "            state=next_state\n",
    "            action=next_action\n",
    "            if render:\n",
    "                self.env.render()\n",
    "            if done:\n",
    "                break\n",
    "        return total_reward,total_steps\n",
    "\n",
    "    def train(self,max_eporch):\n",
    "        for eporch in range(max_eporch):\n",
    "            rewards,steps=self.train_eporch()\n",
    "            if(eporch % 20==0):\n",
    "                print(\"Eporch %03s: steps = %02s , reward = %.1f\"%(eporch,steps,rewards))\n",
    "        self.model.save()\n",
    "    def test(self):\n",
    "#         self.model.load()\n",
    "        self.test_episode(render=True)\n",
    "    \n",
    "    def test_episode(self,render=False):\n",
    "        total_reward = 0\n",
    "        actions = []\n",
    "        state=self.env.reset()\n",
    "        while True:\n",
    "            action=self.model._action(state)\n",
    "            next_state,reward,done,_=self.env.step(action)\n",
    "            \n",
    "            state=next_state\n",
    "            total_reward+=reward\n",
    "            actions.append(action)\n",
    "            if render:\n",
    "                self.env.render()\n",
    "            if done:\n",
    "                break\n",
    "        print('test reward = %.1f' % (total_reward))\n",
    "        print('test action is: ', actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eporch   0: steps =  8 , reward = 0.0\n",
      "Eporch  20: steps =  4 , reward = 0.0\n",
      "Eporch  40: steps =  5 , reward = 0.0\n",
      "Eporch  60: steps = 23 , reward = 0.0\n",
      "Eporch  80: steps =  2 , reward = 0.0\n",
      "Eporch 100: steps =  7 , reward = 1.0\n",
      "Eporch 120: steps =  7 , reward = 1.0\n",
      "Eporch 140: steps = 10 , reward = 1.0\n",
      "Eporch 160: steps =  6 , reward = 1.0\n",
      "Eporch 180: steps =  6 , reward = 1.0\n",
      "Eporch 200: steps =  4 , reward = 0.0\n",
      "Eporch 220: steps =  6 , reward = 1.0\n",
      "Eporch 240: steps =  6 , reward = 1.0\n",
      "Eporch 260: steps =  6 , reward = 1.0\n",
      "Eporch 280: steps =  6 , reward = 1.0\n",
      "Eporch 300: steps =  7 , reward = 1.0\n",
      "Eporch 320: steps =  6 , reward = 1.0\n",
      "Eporch 340: steps =  6 , reward = 1.0\n",
      "Eporch 360: steps =  6 , reward = 1.0\n",
      "Eporch 380: steps =  6 , reward = 1.0\n",
      "Eporch 400: steps = 10 , reward = 1.0\n",
      "Eporch 420: steps =  6 , reward = 1.0\n",
      "Eporch 440: steps =  3 , reward = 0.0\n",
      "Eporch 460: steps =  6 , reward = 1.0\n",
      "Eporch 480: steps =  6 , reward = 1.0\n",
      "./model/SARSA_table.npy saved.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\", is_slippery=False)\n",
    "env = env.unwrapped\n",
    "agent=Agent(env,0.01,0.8,0.1)\n",
    "agent.train(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "test reward = 1.0\n",
      "test action is:  [1, 1, 2, 2, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
