{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"CartPole-v0\")\n",
    "# env = env.unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# for _ in range(10):\n",
    "#     env.render()\n",
    "#     env.step(env.action_space.sample()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q_learning\n",
    "class Q_learning:\n",
    "    def __init__(self,state_dim,action_dim,lr,gamma,e_greed):\n",
    "        self.state_dim=state_dim\n",
    "        self.action_dim=action_dim\n",
    "        self.lr=lr\n",
    "        self.gamma=gamma\n",
    "        self.e_greed=e_greed\n",
    "        self.Q=np.zeros((state_dim,action_dim))\n",
    "        \n",
    "    def _action(self, state):\n",
    "        if np.random.uniform()<self.e_greed:\n",
    "            action=np.random.choice(self.action_dim)\n",
    "        else:\n",
    "            action = self.predict(state)\n",
    "        return action\n",
    "    def predict(self,state):\n",
    "        all_actions=self.Q[state,:]\n",
    "        max_action=np.max(all_actions)\n",
    "        # 防止最大的 Q 值有多个，找出所有最大的 Q，然后再随机选择\n",
    "        # where函数返回一个 array， 每个元素为下标\n",
    "        max_action_list = np.where(all_actions == max_action)[0]\n",
    "        action = np.random.choice(max_action_list)\n",
    "        return action\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        if done:\n",
    "            target=reward\n",
    "        else:\n",
    "            target=reward+np.max(self.Q[next_state,:])*self.gamma\n",
    "        \n",
    "        self.Q[state,action]+=self.lr*(target-self.Q[state,action])\n",
    "        \n",
    "    def save(self):\n",
    "        npy_file = './model/qlearning_table.npy'\n",
    "        np.save(npy_file, self.Q)\n",
    "        print(npy_file + ' saved.')\n",
    "\n",
    "    def load(self, npy_file='./model/qlearning_table.npy'):\n",
    "        self.Q = np.load(npy_file)\n",
    "        print(npy_file + ' loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,env,lr,gamma,e_greed):\n",
    "        self.env=env\n",
    "        self.lr=lr\n",
    "        self.gamma=gamma\n",
    "        self.e_greed=e_greed\n",
    "        self.model=Q_learning(env.observation_space.n,env.action_space.n,lr,gamma,e_greed)\n",
    "    \n",
    "    def _train(self,max_eporch):\n",
    "        \n",
    "        for eporch in range(max_eporch):\n",
    "            ep_reward,ep_steps=self.run_eporch(render=False)\n",
    "            \n",
    "            if eporch % 20==0:\n",
    "                print('Eporch %03s: steps = %02s , reward = %.1f' % (eporch, ep_steps, ep_reward))\n",
    "        self.model.save()\n",
    "        \n",
    "    def _test(self):\n",
    "#         self.model.load()\n",
    "        self.test_episode(render=True)\n",
    "        \n",
    "    def run_eporch(self,render=False):\n",
    "        total_reward=0\n",
    "        total_steps=0\n",
    "        state=self.env.reset()\n",
    "        while True:\n",
    "            action=self.model._action(state)\n",
    "        \n",
    "            next_state,reward,done,_=self.env.step(action)\n",
    "            \n",
    "            self.model.learn(state,action,reward,next_state,done)\n",
    "            \n",
    "            total_reward+=reward\n",
    "            total_steps+=1\n",
    "            state=next_state\n",
    "            if render:self.env.render()\n",
    "            if done:\n",
    "                break\n",
    "        return total_reward,total_steps\n",
    "    def test_episode(self,render=False):\n",
    "        total_reward = 0\n",
    "        actions = []\n",
    "        state=self.env.reset()\n",
    "        while True:\n",
    "            action=self.model._action(state)\n",
    "            next_state,reward,done,_=self.env.step(action)\n",
    "            \n",
    "            state=next_state\n",
    "            total_reward+=reward\n",
    "            actions.append(action)\n",
    "            if render:\n",
    "                self.env.render()\n",
    "            if done:\n",
    "                break\n",
    "        print('test reward = %.1f' % (total_reward))\n",
    "        print('test action is: ', actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\", is_slippery=False)\n",
    "env = env.unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=Agent(env,0.01,0.8,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eporch   0: steps = 15 , reward = 0.0\n",
      "Eporch  20: steps =  5 , reward = 0.0\n",
      "Eporch  40: steps =  9 , reward = 0.0\n",
      "Eporch  60: steps =  5 , reward = 0.0\n",
      "Eporch  80: steps =  8 , reward = 1.0\n",
      "Eporch 100: steps =  8 , reward = 1.0\n",
      "Eporch 120: steps =  6 , reward = 1.0\n",
      "Eporch 140: steps =  6 , reward = 1.0\n",
      "Eporch 160: steps =  6 , reward = 1.0\n",
      "Eporch 180: steps =  6 , reward = 1.0\n",
      "Eporch 200: steps =  6 , reward = 1.0\n",
      "Eporch 220: steps =  6 , reward = 1.0\n",
      "Eporch 240: steps =  6 , reward = 1.0\n",
      "Eporch 260: steps =  6 , reward = 1.0\n",
      "Eporch 280: steps =  2 , reward = 0.0\n",
      "Eporch 300: steps =  6 , reward = 1.0\n",
      "Eporch 320: steps =  8 , reward = 1.0\n",
      "Eporch 340: steps =  6 , reward = 1.0\n",
      "Eporch 360: steps =  6 , reward = 1.0\n",
      "Eporch 380: steps =  6 , reward = 1.0\n",
      "Eporch 400: steps =  6 , reward = 1.0\n",
      "Eporch 420: steps =  6 , reward = 1.0\n",
      "Eporch 440: steps =  7 , reward = 1.0\n",
      "Eporch 460: steps =  6 , reward = 1.0\n",
      "Eporch 480: steps =  6 , reward = 1.0\n",
      "./model/qlearning_table.npy saved.\n"
     ]
    }
   ],
   "source": [
    "agent._train(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/qlearning_table.npy loaded.\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "test reward = 1.0\n",
      "test action is:  [1, 1, 2, 1, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "agent._test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
